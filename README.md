# NanoPoor
NanoGPT-speedrunning for the poor T4 enjoyers

Inspired by [Modded NanoGPT](https://github.com/KellerJordan/modded-nanogpt), I trained a custom GPT I've been working on over at [Dagonet](https://github.com/BambooML/Dagonet), got to the 3.28 val loss, but with some caveats:

 - used GPT-4 tokenizer (think GPT-2 would be better anyway because the models 124M, but CE loss is not comparable)
 - was just a 1B subset of finewebedu10b, not filtered or anything I just processed that much at this time, will probably fix this later

## Runs

