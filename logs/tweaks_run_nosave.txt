--- Attempting Distributed Initialization (TCP Method, Device: cuda) ---
Successfully called init_process_group (TCP) with backend: nccl.
Is distributed initialized after setup? True
--- Finished Distributed Initialization Attempt ---
found vocab_size = 100277 (inside data/tokenized_data/meta.pkl)
Starting run 2yapG8 from scratch
num Muon parameters: 57,824,256
num AdamW parameters: 61,624,832
Using Muon (optimizer 0) and AdamW (optimizer 1)
compiling the model...
compiled
119.449088 M trainable parameters

Starting training loop from iteration 0...
/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
step: 10, train loss: 5.8910, val loss: 5.9025, elapsed time: 2.30 min, dt: 138136.45 ms
step: 20, train loss: 4.0102, val loss: 4.0368, elapsed time: 3.45 min, dt: 69152.66 ms
step: 30, train loss: 4.0095, val loss: 4.0048, elapsed time: 4.28 min, dt: 49494.04 ms
step: 40, train loss: 3.9923, val loss: 3.9634, elapsed time: 5.10 min, dt: 49015.26 ms
step: 50, train loss: 3.9593, val loss: 3.9881, elapsed time: 5.91 min, dt: 49056.54 ms
step: 60, train loss: 3.9213, val loss: 3.9232, elapsed time: 6.73 min, dt: 48822.89 ms
step: 70, train loss: 3.9156, val loss: 3.9319, elapsed time: 7.54 min, dt: 48766.85 ms
step: 80, train loss: 3.8683, val loss: 3.8783, elapsed time: 8.35 min, dt: 48658.99 ms
step: 90, train loss: 3.7697, val loss: 3.7703, elapsed time: 9.16 min, dt: 48777.39 ms
step: 100, train loss: 3.6790, val loss: 3.7020, elapsed time: 9.98 min, dt: 48859.98 ms
step: 110, train loss: 3.6369, val loss: 3.5895, elapsed time: 10.79 min, dt: 48859.54 ms
step: 120, train loss: 3.5343, val loss: 3.5527, elapsed time: 11.60 min, dt: 48677.10 ms
step: 130, train loss: 3.4485, val loss: 3.4703, elapsed time: 12.42 min, dt: 48793.44 ms
step: 140, train loss: 3.3778, val loss: 3.3954, elapsed time: 13.23 min, dt: 48763.12 ms
step: 150, train loss: 3.3642, val loss: 3.3163, elapsed time: 14.04 min, dt: 48821.04 ms
step: 160, train loss: 3.2074, val loss: 3.1957, elapsed time: 14.86 min, dt: 48685.86 ms
