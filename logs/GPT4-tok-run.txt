# First ever run

# tokens used is ctx_len*batch_size*grad_accum*steps = 1024*6*4*570 = 14008320, 14M tokens

found vocab_size = 100277 (inside data/tokenized_data/meta.pkl)
num decayed parameter tensors: 278, with 119,443,712 parameters
num non-decayed parameter tensors: 206, with 203,025 parameters
using fused AdamW: True
starting run 8mqSZf from scratch
compiling the model...
compiled
119.646797 M params
W0331 21:26:50.368000 4777 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode
step: 0, train loss: 11.6878, val loss: 11.6905, elapsed time: 4.2598 min, mfu: 0.00109919, total_flops: 1.8261e+13
/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
step: 10, train loss: 7.5543, val loss: 7.5441, elapsed time: 5.9373 min, mfu: 0.00279129, total_flops: 6.4634e+13
step: 20, train loss: 5.1971, val loss: 5.1872, elapsed time: 6.9738 min, mfu: 0.00451737, total_flops: 1.2286e+14
step: 30, train loss: 4.6162, val loss: 4.5774, elapsed time: 7.9321 min, mfu: 0.00488616, total_flops: 1.5115e+14
step: 40, train loss: 4.1497, val loss: 4.1250, elapsed time: 8.8896 min, mfu: 0.00489007, total_flops: 1.6954e+14
step: 50, train loss: 3.9786, val loss: 4.0142, elapsed time: 9.8746 min, mfu: 0.00475375, total_flops: 1.8307e+14
step: 60, train loss: 3.9823, val loss: 3.9745, elapsed time: 11.7877 min, mfu: 0.00244749, total_flops: 1.1252e+14
step: 70, train loss: 3.9559, val loss: 3.9975, elapsed time: 12.8118 min, mfu: 0.00457232, total_flops: 2.2846e+14
step: 80, train loss: 3.9556, val loss: 3.9723, elapsed time: 14.1266 min, mfu: 0.00356108, total_flops: 1.9619e+14
step: 90, train loss: 3.9474, val loss: 3.9719, elapsed time: 15.1842 min, mfu: 0.00442757, total_flops: 2.6219e+14
step: 100, train loss: 3.9621, val loss: 3.9485, elapsed time: 16.3599 min, mfu: 0.00398247, total_flops: 2.5410e+14
step: 110, train loss: 3.9657, val loss: 3.9476, elapsed time: 17.5582 min, mfu: 0.00390742, total_flops: 2.6757e+14
step: 120, train loss: 3.9218, val loss: 3.9505, elapsed time: 18.6338 min, mfu: 0.00435334, total_flops: 3.1637e+14
step: 130, train loss: 3.9347, val loss: 3.9418, elapsed time: 19.7117 min, mfu: 0.00434395, total_flops: 3.3394e+14
step: 140, train loss: 3.9684, val loss: 3.9282, elapsed time: 20.7854 min, mfu: 0.00436084, total_flops: 3.5350e+14
step: 150, train loss: 3.9149, val loss: 3.9402, elapsed time: 21.9225 min, mfu: 0.00411777, total_flops: 3.5206e+14
step: 160, train loss: 3.9220, val loss: 3.9144, elapsed time: 23.6963 min, mfu: 0.00263982, total_flops: 2.4396e+14
step: 170, train loss: 3.8918, val loss: 3.8444, elapsed time: 24.8278 min, mfu: 0.00413804, total_flops: 4.0068e+14
step: 180, train loss: 3.8807, val loss: 3.8477, elapsed time: 25.7900 min, mfu: 0.00486624, total_flops: 4.8945e+14
step: 190, train loss: 3.8007, val loss: 3.8110, elapsed time: 27.0022 min, mfu: 0.00386283, total_flops: 4.0679e+14
step: 200, train loss: 3.7953, val loss: 3.7890, elapsed time: 28.2383 min, mfu: 0.00378779, total_flops: 4.1715e+14
/content/Dagonet/src/plot.py:31: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(8, 4), dpi=100)
step: 210, train loss: 3.7528, val loss: 3.7398, elapsed time: 29.2736 min, mfu: 0.00452280, total_flops: 5.1636e+14
step: 220, train loss: 3.7527, val loss: 3.7445, elapsed time: 30.3741 min, mfu: 0.00425478, total_flops: 5.0402e+14
step: 230, train loss: 3.6900, val loss: 3.7211, elapsed time: 32.3297 min, mfu: 0.00239432, total_flops: 3.0189e+14
step: 240, train loss: 3.6982, val loss: 3.6794, elapsed time: 33.4205 min, mfu: 0.00429260, total_flops: 5.5950e+14
step: 250, train loss: 3.6778, val loss: 3.6606, elapsed time: 34.4805 min, mfu: 0.00441723, total_flops: 5.9400e+14
step: 260, train loss: 3.6398, val loss: 3.6378, elapsed time: 35.4419 min, mfu: 0.00487057, total_flops: 6.7323e+14
step: 270, train loss: 3.6196, val loss: 3.6187, elapsed time: 36.6087 min, mfu: 0.00401265, total_flops: 5.7290e+14
step: 280, train loss: 3.5822, val loss: 3.6158, elapsed time: 37.7452 min, mfu: 0.00412023, total_flops: 6.0652e+14
step: 290, train loss: 3.5738, val loss: 3.5825, elapsed time: 38.9017 min, mfu: 0.00404859, total_flops: 6.1424e+14
step: 300, train loss: 3.5570, val loss: 3.5395, elapsed time: 40.0013 min, mfu: 0.00425832, total_flops: 6.6432e+14
step: 310, train loss: 3.5456, val loss: 3.5815, elapsed time: 41.3260 min, mfu: 0.00353467, total_flops: 5.6969e+14
step: 320, train loss: 3.5627, val loss: 3.5953, elapsed time: 42.3248 min, mfu: 0.00468803, total_flops: 7.7384e+14
step: 330, train loss: 3.5262, val loss: 3.5836, elapsed time: 43.4853 min, mfu: 0.00403454, total_flops: 6.8423e+14
step: 340, train loss: 3.5075, val loss: 3.5125, elapsed time: 44.7525 min, mfu: 0.00369516, total_flops: 6.4493e+14
step: 350, train loss: 3.5361, val loss: 3.5139, elapsed time: 45.8161 min, mfu: 0.00440234, total_flops: 7.8662e+14
step: 360, train loss: 3.4930, val loss: 3.5195, elapsed time: 46.8953 min, mfu: 0.00433842, total_flops: 7.9346e+14
step: 370, train loss: 3.4761, val loss: 3.4916, elapsed time: 47.9731 min, mfu: 0.00434452, total_flops: 8.1284e+14
step: 380, train loss: 3.4725, val loss: 3.4822, elapsed time: 49.1806 min, mfu: 0.00387787, total_flops: 7.4379e+14
step: 390, train loss: 3.5043, val loss: 3.4283, elapsed time: 50.2219 min, mfu: 0.00449652, total_flops: 8.8071e+14
step: 400, train loss: 3.4199, val loss: 3.4215, elapsed time: 51.3271 min, mfu: 0.00423647, total_flops: 8.4804e+14
step: 410, train loss: 3.4194, val loss: 3.4291, elapsed time: 52.4343 min, mfu: 0.00422906, total_flops: 8.6482e+14
step: 420, train loss: 3.4330, val loss: 3.3939, elapsed time: 53.5508 min, mfu: 0.00419388, total_flops: 8.7588e+14
step: 430, train loss: 3.3952, val loss: 3.4142, elapsed time: 54.7765 min, mfu: 0.00382009, total_flops: 8.1608e+14
step: 440, train loss: 3.3877, val loss: 3.4125, elapsed time: 55.7788 min, mfu: 0.00467162, total_flops: 1.0163e+15
step: 450, train loss: 3.3836, val loss: 3.3682, elapsed time: 56.8280 min, mfu: 0.00446271, total_flops: 9.8907e+14
step: 460, train loss: 3.3667, val loss: 3.4045, elapsed time: 58.0875 min, mfu: 0.00371753, total_flops: 8.4217e+14
step: 470, train loss: 3.3659, val loss: 3.4039, elapsed time: 59.4221 min, mfu: 0.00350840, total_flops: 8.1306e+14
step: 480, train loss: 3.3528, val loss: 3.3764, elapsed time: 60.7892 min, mfu: 0.00342520, total_flops: 8.1204e+14
step: 490, train loss: 3.3788, val loss: 3.3866, elapsed time: 61.9427 min, mfu: 0.00405923, total_flops: 9.8061e+14
step: 500, train loss: 3.3362, val loss: 3.3539, elapsed time: 62.9484 min, mfu: 0.00465548, total_flops: 1.1429e+15
step: 510, train loss: 3.3687, val loss: 3.3481, elapsed time: 64.0334 min, mfu: 0.00431545, total_flops: 1.0777e+15
step: 520, train loss: 3.3415, val loss: 3.3037, elapsed time: 65.2529 min, mfu: 0.00383981, total_flops: 9.7718e+14
step: 530, train loss: 3.3251, val loss: 3.3111, elapsed time: 66.3568 min, mfu: 0.00424145, total_flops: 1.0977e+15
step: 540, train loss: 3.3138, val loss: 3.3068, elapsed time: 67.4962 min, mfu: 0.00410959, total_flops: 1.0818e+15
step: 550, train loss: 3.2846, val loss: 3.3099, elapsed time: 68.4892 min, mfu: 0.00471533, total_flops: 1.2595e+15
step: 560, train loss: 3.2775, val loss: 3.2866, elapsed time: 69.6459 min, mfu: 0.00404811, total_flops: 1.0995e+15
step: 570, train loss: 3.2876, val loss: 3.2631, elapsed time: 70.6161 min, mfu: 0.00482616, total_flops: 1.3291e+15
